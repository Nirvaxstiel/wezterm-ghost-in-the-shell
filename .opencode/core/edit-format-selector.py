"""
Edit Format Selector
Auto-detects model and selects optimal edit format

Purpose: +20-61% edit success rate through intelligent format selection
Based on: "The Harness Problem" research + telemetry data

Key Features:
- Model auto-detection from environment
- Telemetry-driven format selection (learn from usage)
- Automatic fallback chain on failure
- Model-specific format recommendations
"""

import os
import re
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple


class EditFormat(Enum):
    """Supported edit formats"""

    STR_REPLACE = "str_replace"
    STR_REPLACE_FUZZY = "str_replace_fuzzy"
    APPLY_PATCH = "apply_patch"
    HASHLINE = "hashline"


class EditFormatSelector:
    """Select optimal edit format based on model and telemetry"""

    # Model-specific format recommendations (heuristics)
    MODEL_FORMATS = {
        "claude": EditFormat.STR_REPLACE,
        "gemini": EditFormat.STR_REPLACE_FUZZY,
        "gpt": EditFormat.APPLY_PATCH,
        "openai": EditFormat.APPLY_PATCH,
        "grok": EditFormat.HASHLINE,  # Weak models benefit most from hashline
        "glm": EditFormat.HASHLINE,  # +8-14% improvement for GLM
        "claude-opus": EditFormat.STR_REPLACE,
        "claude-sonnet": EditFormat.STR_REPLACE,
        "claude-haiku": EditFormat.STR_REPLACE,
        "gpt-4": EditFormat.APPLY_PATCH,
        "gpt-4-turbo": EditFormat.APPLY_PATCH,
        "gpt-4o": EditFormat.APPLY_PATCH,
        "gemini-1.5": EditFormat.STR_REPLACE_FUZZY,
        "gemini-2.0": EditFormat.STR_REPLACE_FUZZY,
        "gemini-2.5": EditFormat.STR_REPLACE_FUZZY,
    }

    # Fallback chains (order to try if primary fails)
    FALLBACK_CHAINS = {
        EditFormat.STR_REPLACE: [EditFormat.HASHLINE, EditFormat.APPLY_PATCH],
        EditFormat.STR_REPLACE_FUZZY: [
            EditFormat.STR_REPLACE,
            EditFormat.HASHLINE,
            EditFormat.APPLY_PATCH,
        ],
        EditFormat.APPLY_PATCH: [EditFormat.STR_REPLACE, EditFormat.HASHLINE],
        EditFormat.HASHLINE: [EditFormat.STR_REPLACE, EditFormat.APPLY_PATCH],
    }

    # Format descriptions
    FORMAT_DESCRIPTIONS = {
        EditFormat.STR_REPLACE: {
            "name": "str_replace",
            "best_for": "Claude, Gemini (exact matches)",
            "description": "Claude default format - exact string matching",
        },
        EditFormat.STR_REPLACE_FUZZY: {
            "name": "str_replace_fuzzy",
            "best_for": "Gemini (with fuzzy whitespace)",
            "description": "str_replace with fuzzy whitespace matching",
        },
        EditFormat.APPLY_PATCH: {
            "name": "apply_patch",
            "best_for": "GPT, OpenAI models",
            "description": "OpenAI-style diff format (<<<<<, =======, >>>>>)",
        },
        EditFormat.HASHLINE: {
            "name": "hashline",
            "best_for": "Grok, GLM, smaller models",
            "description": "Content-based line anchoring (+8-61% improvement)",
        },
    }

    def __init__(self, telemetry_callback: Optional[Callable] = None):
        """
        Initialize edit format selector

        Args:
            telemetry_callback: Optional callback to get telemetry data
        """
        self.telemetry_callback = telemetry_callback
        self._cached_model = None
        self._cached_selection = None

    def detect_model(self) -> str:
        """
        Auto-detect model from environment

        Returns:
            Lowercase model identifier (e.g., 'glm-4.7', 'claude-opus-4')
        """
        if self._cached_model is not None:
            return self._cached_model

        # Priority 1: Check dedicated model environment variables
        for env_var in ["LLM_MODEL", "MODEL", "MODEL_NAME"]:
            model = os.getenv(env_var)
            if model:
                self._cached_model = model.lower().strip()
                return self._cached_model

        # Priority 2: Check common API-specific env vars
        api_env_vars = {
            "ANTHROPIC_MODEL": "claude",
            "OPENAI_MODEL": "gpt",
            "GOOGLE_MODEL": "gemini",
            "XAI_MODEL": "grok",
            "GLM_MODEL": "glm",
        }

        for env_var, model_prefix in api_env_vars.items():
            model = os.getenv(env_var)
            if model:
                self._cached_model = model.lower().strip()
                return self._cached_model

        # Priority 3: Try to infer from system
        model = self._infer_model_from_system()
        if model:
            self._cached_model = model
            return self._cached_model

        # Default: unknown
        self._cached_model = "unknown"
        return self._cached_model

    def _infer_model_from_system(self) -> Optional[str]:
        """
        Try to infer model from system context

        Returns:
            Model identifier or None
        """
        # Try to read from common config files
        config_paths = [
            ".env",
            ".llmrc",
            ".openai/config",
            "~/.anthropic/config",
        ]

        for path in config_paths:
            expanded_path = os.path.expanduser(path)
            if os.path.exists(expanded_path):
                try:
                    with open(expanded_path, "r") as f:
                        content = f.read()

                    # Look for model patterns
                    patterns = [
                        r'MODEL\s*=\s*[\'"]?([^\'"\s]+)[\'"]?',
                        r'model\s*:\s*[\'"]?([^\'"\s]+)[\'"]?',
                        r"--model\s+([^\s]+)",
                    ]

                    for pattern in patterns:
                        match = re.search(pattern, content, re.IGNORECASE)
                        if match:
                            return match.group(1).lower().strip()
                except Exception:
                    continue

        return None

    def select_format(
        self, model: Optional[str] = None, force_heuristic: bool = False
    ) -> Tuple[EditFormat, float, str]:
        """
        Select optimal format for model

        Args:
            model: Model identifier (uses auto-detection if None)
            force_heuristic: Force heuristic selection (ignore telemetry)

        Returns:
            Tuple of (format, confidence, reason)
        """
        if model is None:
            model = self.detect_model()

        # Priority 1: Use telemetry data if available
        if not force_heuristic and self.telemetry_callback:
            telemetry_result = self._select_from_telemetry(model)
            if telemetry_result is not None:
                return telemetry_result

        # Priority 2: Use heuristics
        return self._select_from_heuristic(model)

    def _select_from_telemetry(
        self, model: str
    ) -> Optional[Tuple[EditFormat, float, str]]:
        """
        Select format based on telemetry data

        Args:
            model: Model identifier

        Returns:
            Tuple of (format, confidence, reason) or None if no telemetry
        """
        try:
            # Check if telemetry callback is available
            if self.telemetry_callback is None:
                return None

            # Get telemetry success rates
            success_rates = self.telemetry_callback(model)

            if not success_rates:
                return None

            # Find format with highest success rate
            best_format_str = max(success_rates, key=success_rates.get)
            best_rate = success_rates[best_format_str]

            # Convert to EditFormat enum
            try:
                best_format = EditFormat(best_format_str)
                return (
                    best_format,
                    best_rate,
                    f"Telemetry-based selection: {best_format_str} has {best_rate:.1%} success rate",
                )
            except ValueError:
                return None
        except Exception:
            # Telemetry unavailable or error - fall back to heuristics
            return None

    def _select_from_heuristic(self, model: str) -> Tuple[EditFormat, float, str]:
        """
        Select format based on model heuristics

        Args:
            model: Model identifier

        Returns:
            Tuple of (format, confidence, reason)
        """
        # Check for exact model match
        if model in self.MODEL_FORMATS:
            format_type = self.MODEL_FORMATS[model]
            return (
                format_type,
                0.95,  # High confidence for exact match
                f"Heuristic: Model '{model}' maps to {format_type.value}",
            )

        # Check for partial match (e.g., 'claude' matches 'claude-opus-4')
        for pattern, format_type in self.MODEL_FORMATS.items():
            if pattern in model:
                return (
                    format_type,
                    0.85,  # Medium-high confidence for partial match
                    f"Heuristic: Model '{model}' contains '{pattern}' -> {format_type.value}",
                )

        # Default: use hashline (good for unknown models)
        return (
            EditFormat.HASHLINE,
            0.5,  # Low confidence for default
            f"Heuristic: Unknown model '{model}', defaulting to hashline",
        )

    def get_fallback_chain(self, format_type: EditFormat) -> List[EditFormat]:
        """
        Get fallback chain for a format

        Args:
            format_type: Primary format

        Returns:
            List of formats to try in order
        """
        return self.FALLBACK_CHAINS.get(format_type, [])

    def execute_with_retry(
        self,
        filepath: str,
        edit_op: Dict[str, Any],
        max_attempts: int = 3,
        edit_callback: Optional[Callable] = None,
        model: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Execute edit with automatic format retry

        Args:
            filepath: Path to file to edit
            edit_op: Edit operation details
            max_attempts: Maximum number of format attempts
            edit_callback: Function to execute edit (signature: format, filepath, edit_op)
            model: Model identifier (uses auto-detection if None)

        Returns:
            Dictionary with execution result
        """
        if model is None:
            model = self.detect_model()

        # Select primary format
        primary_format, confidence, reason = self.select_format(model)

        results = []
        formats_to_try = [primary_format] + self.get_fallback_chain(primary_format)

        # Try formats in order
        for attempt in range(min(max_attempts, len(formats_to_try))):
            format_to_try = formats_to_try[attempt]
            format_desc = self.FORMAT_DESCRIPTIONS[format_to_try]

            try:
                # Execute edit
                if edit_callback is None:
                    # Default callback using hashline processor
                    result = self._default_edit_callback(
                        format_to_try, filepath, edit_op
                    )
                else:
                    result = edit_callback(format_to_try, filepath, edit_op)

                # Success!
                return {
                    "success": True,
                    "format_used": format_to_try.value,
                    "attempts": attempt + 1,
                    "confidence": confidence,
                    "reason": reason,
                    "format_description": format_desc,
                    "result": result,
                }
            except Exception as e:
                # Failed - try next format
                error_msg = str(e)
                results.append(
                    {
                        "format": format_to_try.value,
                        "error": error_msg,
                        "attempt": attempt + 1,
                    }
                )
                continue

        # All formats failed
        return {
            "success": False,
            "formats_tried": [f.value for f in formats_to_try[:max_attempts]],
            "errors": [r["error"] for r in results],
            "primary_format": primary_format.value,
            "model": model,
            "reason": reason,
            "attempts": len(results),
            "error": "All edit formats failed",
        }

    def _default_edit_callback(
        self, format_type: EditFormat, filepath: str, edit_op: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Default edit callback (uses hashline processor if needed)

        Args:
            format_type: Format to use
            filepath: Path to file
            edit_op: Edit operation details

        Returns:
            Edit result

        Raises:
            Exception if edit fails
        """
        if format_type == EditFormat.HASHLINE:
            # Use hashline processor
            try:
                # Import hashline processor dynamically to avoid circular imports
                import importlib.util

                hashline_path = os.path.join(
                    os.path.dirname(os.path.dirname(__file__)),
                    "tools",
                    "hashline-processor.py",
                )

                spec = importlib.util.spec_from_file_location(
                    "hashline_processor", hashline_path
                )
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    HashlineProcessor = module.HashlineProcessor

                    processor = HashlineProcessor()
                    search_text = edit_op.get("oldString", "")
                    new_content = edit_op.get("newString", "")

                    # Find line by content
                    hash_ref = processor.find_hash_line(filepath, search_text)
                    if not hash_ref:
                        raise ValueError(
                            f"Could not find line with content: {search_text}"
                        )

                    # Apply edit
                    success = processor.apply_hashline_edit(
                        filepath=filepath, target_hash=hash_ref, new_content=new_content
                    )

                    return {"success": success}
                else:
                    raise ValueError("Could not load hashline processor spec")

            except Exception as e:
                raise ValueError(f"Hashline processor error: {str(e)}")

        elif format_type in [EditFormat.STR_REPLACE, EditFormat.STR_REPLACE_FUZZY]:
            # Use Edit tool (str_replace format)
            # This would be integrated with actual Edit tool
            # For now, we'll use hashline as a proxy
            try:
                # Import hashline processor dynamically
                import importlib.util

                hashline_path = os.path.join(
                    os.path.dirname(os.path.dirname(__file__)),
                    "tools",
                    "hashline-processor.py",
                )

                spec = importlib.util.spec_from_file_location(
                    "hashline_processor", hashline_path
                )
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    HashlineProcessor = module.HashlineProcessor

                    processor = HashlineProcessor()
                    search_text = edit_op.get("oldString", "")
                    new_content = edit_op.get("newString", "")

                # Find line by content
                hash_ref = processor.find_hash_line(filepath, search_text)
                if not hash_ref:
                    raise ValueError(f"Could not find line with content: {search_text}")

                # Apply edit
                success = processor.apply_hashline_edit(
                    filepath=filepath, target_hash=hash_ref, new_content=new_content
                )

                return {"success": success}

            except ImportError:
                raise ValueError("Hashline processor not available")

        elif format_type == EditFormat.APPLY_PATCH:
            # Apply patch format
            # This would be integrated with Edit tool
            # For now, we'll use hashline as a proxy
            raise NotImplementedError(
                "apply_patch format requires Edit tool integration"
            )

        else:
            raise ValueError(f"Unknown format: {format_type}")

    def get_format_description(self, format_type: EditFormat) -> Dict[str, str]:
        """
        Get description for a format

        Args:
            format_type: Format type

        Returns:
            Dictionary with format details
        """
        return self.FORMAT_DESCRIPTIONS.get(format_type, {})

    def get_model_recommendation(self, model: Optional[str] = None) -> Dict[str, Any]:
        """
        Get recommended format for a model

        Args:
            model: Model identifier (uses auto-detection if None)

        Returns:
            Dictionary with recommendation details
        """
        if model is None:
            model = self.detect_model()

        format_type, confidence, reason = self.select_format(
            model, force_heuristic=True
        )
        format_desc = self.get_format_description(format_type)
        fallback_chain = [f.value for f in self.get_fallback_chain(format_type)]

        return {
            "model": model,
            "recommended_format": format_type.value,
            "confidence": confidence,
            "reason": reason,
            "format_description": format_desc,
            "fallback_chain": fallback_chain,
            "notes": self._get_model_notes(model, format_type),
        }

    def _get_model_notes(self, model: str, format_type: EditFormat) -> List[str]:
        """
        Get model-specific notes

        Args:
            model: Model identifier
            format_type: Recommended format

        Returns:
            List of notes
        """
        notes = []

        # Known strengths/weaknesses
        if "grok" in model:
            if format_type == EditFormat.HASHLINE:
                notes.append(
                    "Grok shows 10x improvement with hashline format (6.7% -> 68.3%)"
                )
            else:
                notes.append(
                    "WARNING: Grok struggles with traditional formats - consider hashline"
                )

        elif "glm" in model:
            if format_type == EditFormat.HASHLINE:
                notes.append(
                    "GLM shows +8-14% improvement with hashline (46-50% -> 54-64%)"
                )
            else:
                notes.append(
                    "WARNING: GLM has poor str_replace performance (46-50% failure)"
                )

        elif "claude" in model:
            notes.append("Claude excels with str_replace format (92-95% success rate)")
            notes.append("Hashline may offer minor improvements for complex edits")

        elif "gpt" in model:
            notes.append("GPT works best with apply_patch format (91-94% success rate)")

        elif "gemini" in model:
            notes.append("Gemini works best with str_replace_fuzzy (93% success rate)")
            notes.append("Good whitespace handling allows fuzzy matching")

        return notes


# Singleton instance
_selector_instance = None
_instance_lock = None  # Lazy import


def get_edit_selector(
    telemetry_callback: Optional[Callable] = None,
) -> EditFormatSelector:
    """
    Get singleton edit format selector instance

    Args:
        telemetry_callback: Optional callback to get telemetry data

    Returns:
        EditFormatSelector instance
    """
    global _selector_instance

    if _selector_instance is None:
        _selector_instance = EditFormatSelector(telemetry_callback=telemetry_callback)

    return _selector_instance


# CLI interface for testing
if __name__ == "__main__":
    import argparse
    import sys

    parser = argparse.ArgumentParser(description="Edit Format Selector")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Detect command
    detect_parser = subparsers.add_parser("detect", help="Auto-detect model")
    detect_parser.add_argument(
        "--env", action="store_true", help="Show environment variables checked"
    )

    # Recommend command
    recommend_parser = subparsers.add_parser(
        "recommend", help="Get format recommendation"
    )
    recommend_parser.add_argument("--model", help="Model identifier (optional)")

    # Test command
    test_parser = subparsers.add_parser("test", help="Test edit with retry")
    test_parser.add_argument("filepath", help="File path to edit")
    test_parser.add_argument("--model", help="Model identifier (optional)")
    test_parser.add_argument(
        "--max-attempts", type=int, default=3, help="Max format attempts"
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    selector = EditFormatSelector()

    if args.command == "detect":
        model = selector.detect_model()
        print(f"Detected model: {model}")

        if args.env:
            print("\nChecked environment variables:")
            for env_var in [
                "LLM_MODEL",
                "MODEL",
                "MODEL_NAME",
                "ANTHROPIC_MODEL",
                "OPENAI_MODEL",
            ]:
                value = os.getenv(env_var)
                if value:
                    print(f"  {env_var}={value}")

    elif args.command == "recommend":
        model = args.model or selector.detect_model()
        recommendation = selector.get_model_recommendation(model)

        print(f"\n=== Format Recommendation for {model} ===")
        print(f"Format: {recommendation['recommended_format']}")
        print(f"Confidence: {recommendation['confidence']:.1%}")
        print(f"Reason: {recommendation['reason']}")
        print(f"\nDescription: {recommendation['format_description']['description']}")
        print(f"Best for: {recommendation['format_description']['best_for']}")

        if recommendation["notes"]:
            print("\nNotes:")
            for note in recommendation["notes"]:
                print(f"  - {note}")

        print(f"\nFallback chain: {' -> '.join(recommendation['fallback_chain'])}")

    elif args.command == "test":
        print("Test mode not yet implemented")
        print("Use with Edit tool integration")
